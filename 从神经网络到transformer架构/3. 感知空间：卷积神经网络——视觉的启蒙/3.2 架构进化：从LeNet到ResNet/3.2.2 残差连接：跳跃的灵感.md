#### [类比] ResNet的“残差块”引入了一条“高速公路”（跳跃连接）。如果某一层什么也没学到，那么它至少可以轻松地把自己输出变成0，让信息无损地通过高速公路跳过它。这相当于告诉网络：“如果你不知道怎么做，那就什么都别做”，从而让训练超深网络成为可能。


#### [影响] 残差连接的思想是如此强大，它几乎成为了现代深度学习架构（包括Transformer）的标准配置。这种“捷径”思维还能应用在哪些看似不可能的地方？


#### [谜题] 为什么说ResNet在某种程度上是在学习“输出”与“输入”之间的“差异”（残差），而不是直接学习“输出”？


## 摘要


## 要点

- 
- 
- 

## 链接

- [[3.2 架构进化：从LeNet到ResNet MOC]]
