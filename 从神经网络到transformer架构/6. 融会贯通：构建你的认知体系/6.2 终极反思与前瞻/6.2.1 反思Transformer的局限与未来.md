#### [谜题] Transformer的注意力机制计算复杂度是序列长度的平方（O(n²)），这使其难以处理极长序列（例如一整本书）。有哪些新兴技术（如Linear Attention, Longformer）在试图解决这个问题？


#### [连接] 当前火热的Multimodal（多模态）模型和 Generative AI（生成式AI），其核心是在Transformer的基础上做了哪些扩展？它们是如何处理图像、文本、音频之间的“跨模态注意力”的？


#### [故事] 回顾这段从`y = ax + b`到GPT-4的旅程。下一个范式级的突破，可能会在哪个环节发生？是更高效的注意力机制，全新的优化算法，还是完全不同的生物启发计算模型？


## 摘要


## 要点

- 
- 
- 

## 链接

- [[6.2 终极反思与前瞻 MOC]]
