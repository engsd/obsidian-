#### [类比] 把一句话中的每个词想象成一个在图书馆（序列）里找资料的研究员。每个词同时扮演三个角色：


#### [规律] 点积注意力公式 `Attention(Q, K, V) = softmax(QK^T / √d_k) V` 中的每一步在数学上完成了什么？为什么要除以 `√d_k`？（缩放点积，防止梯度消失）


#### [影响] 在翻译“The animal didn't cross the street because it was too tired”时，“it”这个词的Query会如何高概率地匹配到“animal”的Key，而不是“street”的Key？这种精准的指代消解是RNN难以做到的。


## 摘要


## 要点

- 
- 
- 

## 链接

- [[5.1 自注意力机制：全局关联的瞬间洞察 MOC]]
