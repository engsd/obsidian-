#### [故事] RNN必须像串行计算一样一步一步处理序列，而Transformer的注意力机制可以一次性计算所有词对之间的关系（形成一个注意力矩阵），这使其能够充分利用GPU进行大规模并行计算，训练速度比RNN快了一个数量级。


## 摘要


## 要点

- 
- 
- 

## 链接

- [[5.1 自注意力机制：全局关联的瞬间洞察 MOC]]
