#### [类比] 自注意力机制本身是“无序”的（置换不变性）。我们需要一种方式告诉模型“我是第一个词，你是第五个词”。位置编码就像是给每个词发一个代表其座次的“号码牌”，模型通过这个号码牌来理解顺序。


#### [规律] 为什么Transformer选择使用正弦和余弦函数来生成位置编码，而不是简单的整数？这种方案如何让模型更容易地学会“关注相对位置”？


## 摘要


## 要点

- 
- 
- 

## 链接

- [[5.2 架构大成：编码器与解码器 MOC]]
