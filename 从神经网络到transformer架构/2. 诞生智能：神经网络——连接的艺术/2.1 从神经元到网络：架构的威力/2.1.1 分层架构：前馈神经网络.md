#### [类比] 将神经网络视为一个多层的“谣言加工厂”：输入层是听到的原始谣言，每个隐藏层都在对谣言进行加工和扭曲，输出层则是最终产出的离奇版本。我们如何训练这个工厂，让它的最终产出是我们想要的“真相”？


#### [规律] 为什么“深度”（层数）比“宽度”（每层的神经元数）更重要？深度网络是如何逐层构建出从边缘->轮廓->部件->整体（例如：像素->边缘->眼睛鼻子->人脸）的层次化特征的？


#### [谜题] Universal Approximation Theorem（万能近似定理）说一个足够宽的网络可以近似任何函数。那为什么我们还需要深度网络？深度解决了什么“足够宽”也无法解决的问题？（计算效率、参数效率、泛化能力）


## 摘要


## 要点

- 
- 
- 

## 链接

- [[2.1 从神经元到网络：架构的威力 MOC]]
