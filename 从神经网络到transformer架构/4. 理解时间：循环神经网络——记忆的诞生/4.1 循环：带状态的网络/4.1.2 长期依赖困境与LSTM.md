#### [谜题] 基础RNN的“记忆”非常短暂，它很难记住很久以前的信息（例如段首的第一个词）。这是因为在反向传播时，梯度同样会随着时间步长而指数级消失（时间上的梯度消失）。如何解决？


#### [规律] LSTM（长短期记忆网络）通过引入“门控机制”（输入门、遗忘门、输出门）和“细胞状态”（一条贯穿时间的“记忆高速公路”）来精准控制信息的遗忘、存储和输出。它是如何像一个小型内存管理器一样工作的？


#### [连接] LSTM的“门”在概念上是否类似于数字电路中的逻辑门？它们是如何通过sigmoid函数（输出0-1，模拟开/关）和逐元素乘法来实现的？


## 摘要


## 要点

- 
- 
- 

## 链接

- [[4.1 循环：带状态的网络 MOC]]
