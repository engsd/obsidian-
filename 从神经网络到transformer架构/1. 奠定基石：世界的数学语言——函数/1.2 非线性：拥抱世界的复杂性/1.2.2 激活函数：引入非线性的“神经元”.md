#### [规律] Sigmoid函数如何将一个无限的输入范围，“挤压”到一个0到1的有限区间？这种“挤压”模拟了生物神经元的什么特性（放电/不放电）？


#### [连接] ReLU函数（max(0, x)）如此简单，为何却在深度学习中几乎取代了Sigmoid？它的“死区”（负数输出为0）带来了什么意想不到的好处和问题？


#### [故事] 想象一个由千万个微小ReLU函数组成的网络，它们的组合如何能够描绘出图像中一只猫的复杂轮廓？


## 摘要


## 要点

- 
- 
- 

## 链接

- [[1.2 非线性：拥抱世界的复杂性 MOC]]
